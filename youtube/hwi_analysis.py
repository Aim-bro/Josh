# %% installs
!pip install konlpy jpype1
!pip install wordcloud
!pip install matplotlib
!pip install -U seaborn
# %% imports
import os
import pandas as pd
import seaborn as sns
import re, collections
import torch
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline
from collections import Counter
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
# %%
folder = 'data/comments'
files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]

dfs = []
for f in files:
    df = pd.read_csv(f)
    dfs.append(df)
    
all_comments_df = pd.concat(dfs, ignore_index=True)
# %%
print(all_comments_df.info())
print(all_comments_df.head())

# %%
print(all_comments_df[all_comments_df.isnull().any(axis=1)])
all_comments_df = all_comments_df.dropna()
print(all_comments_df.info())
print(all_comments_df.head())
# %% merge

all_comments_labeled = all_comments_df.merge(
    pd.read_csv("data/meta/hwi_videos_labeled.csv", usecols=["video_id","label_rule"]).drop_duplicates("video_id"),
    on="video_id", how="left"
)

all_comments_labeled.to_csv("data/meta/all_comments_labeled.csv", index=False, encoding="utf-8-sig")

# %% A) ëŒ“ê¸€ ë°˜ì‘ ê¸ì •/ì¤‘ë¦½/ë¶€ì • ë¼ë²¨ë§

BASE_LEXICON = set(["ì¢‹ë‹¤","ìµœê³ ","ëŒ€ë°•","ë³„ë¡œ","ìµœì•…","ì§œì¦","í™”ë‚¨"])
SLANG_RE = re.compile(r"(ã…‹ã…‹+|ã…ã…+|ã… +|ã…œ+|ã„·ã„·+|ã„¹ã…‡|ã…‡ã…ˆ|ã„±ã„±|ë…¸ì¼|í•µ.*|ë¯¸ì³¤|ë ˆì „ë“œ|ê°œ\w+)")
POS_HINTS = ["ã…‹ã…‹","ã…ã…","ğŸ˜Š","ğŸ˜","ğŸ‘","ë ˆì „ë“œ","ì§€ë¦¬","ã„¹ã…‡","ì¡´ë²„ì„±ê³µ"]
NEG_HINTS = ["ã… ã… ","ã…œã…œ","ğŸ˜¡","ğŸ¤®","ğŸ‘","ë…¸ì¼","í˜","ë¹¡ì¹¨","ê·¹í˜"]

POS_DICT = {"ë ˆì „ë“œ","ì§€ë¦¬ë„¤","ê°“","ê¿€","í˜œì","ê°œì´ë“","ì‚¬ë‘í•´","ìµì˜¤","ìµœê³ ","ëŒ€ë°•","ê·€ì—¬ì›Œ"}
NEG_DICT = {"ë…¸ì¼","ê·¹í˜","êµ¬ë¦¼","ë¹¡ì¹¨","ë¶•ê´´","ë§í–ˆ","ì‚¬ê¸°","í˜ì˜¤","ì—­ê²¹","ìµœì•…","ë³„ë¡œ","ì§œì¦","ì‹¤ë§"}

LAUGH = re.compile(r"(ã…‹ã…‹+|ã…ã…+)")
NEGATOR = re.compile(r"(ì•ˆ|ëª»|ë³„ë¡œ|ì•„ë‹Œ)")


def extract_slang_candidates(texts, topn=200, min_freq=5):
    freq = collections.Counter()
    for t in texts:
        for tok in re.findall(r"[ê°€-í£A-Za-z0-9]+|[ã…‹ã…ã… ã…œã„·]+", str(t)):
            if tok not in BASE_LEXICON and (SLANG_RE.search(tok) or len(tok) <= 4):
                freq[tok] += 1
    return [w for w, c in freq.most_common(topn) if c >= min_freq]


def weak_polarity_score(texts, candidates):
    co_pos, co_neg, cnt = collections.Counter(), collections.Counter(), collections.Counter()
    for t in texts:
        t = str(t)
        present = {w for w in candidates if w in t}
        pos_hit = any(h in t for h in POS_HINTS)
        neg_hit = any(h in t for h in NEG_HINTS)
        for w in present:
            cnt[w] += 1
            if pos_hit: co_pos[w] += 1
            if neg_hit: co_neg[w] += 1
    scores = {}
    for w in candidates:
        p = (co_pos[w] + 1) / (cnt[w] + 2)
        n = (co_neg[w] + 1) / (cnt[w] + 2)
        scores[w] = p - n
    return scores


def classify_sentiment(text):
    t = str(text)
    pos = any(w in t for w in POS_DICT) or bool(LAUGH.search(t))
    neg = any(w in t for w in NEG_DICT)
    if pos and NEGATOR.search(t):  # "ì•ˆ/ëª»/ë³„ë¡œ/ì•„ë‹Œ" ë“± ë¶€ì •ì–´ê°€ ê¸ì •í‘œí˜„ê³¼ í•¨ê»˜ ë‚˜ì˜¤ë©´ ë’¤ì§‘ê¸°
        pos, neg = False, True
    if pos and not neg: return "ê¸ì •"
    if neg and not pos: return "ë¶€ì •"
    return "ì¤‘ë¦½"



CANDIDATE_MODELS = [
    ("tabularisai/multilingual-sentiment-analysis", "3class"),  # negative/neutral/positive
    ("daekeun-ml/koelectra-small-v3-nsmc", "binary"),          # neg/pos
    ("sangrimlee/bert-base-multilingual-cased-nsmc", "binary") # neg/pos
]

def load_sentiment_pipeline():
    last_err = None
    for mid, mtype in CANDIDATE_MODELS:
        try:
            tok = AutoTokenizer.from_pretrained(mid)
            mdl = AutoModelForSequenceClassification.from_pretrained(mid)
            pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True)
            return pipe, mtype
        except Exception as e:
            last_err = e
            continue
    raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {last_err}")

pipe, MODEL_TYPE = load_sentiment_pipeline()
LABEL_MAP_3 = {"negative":"ë¶€ì •", "neutral":"ì¤‘ë¦½", "positive":"ê¸ì •"}

def refine_with_plm(text, neutral_band=0.15):
    scores = pipe(str(text))[0]  # [{'label': 'positive', 'score': 0.7}, ...]
    if MODEL_TYPE == "3class":
        label = max(scores, key=lambda x: x["score"])["label"].lower()
        return LABEL_MAP_3.get(label, "ì¤‘ë¦½")
    
    pos = next((s["score"] for s in scores if "pos" in s["label"].lower()), None)
    neg = next((s["score"] for s in scores if "neg" in s["label"].lower()), None)
    if pos is None or neg is None:
        label = max(scores, key=lambda x: x["score"])["label"].lower()
        return "ê¸ì •" if "pos" in label else "ë¶€ì •"
    if abs(pos - neg) < neutral_band:
        return "ì¤‘ë¦½"
    return "ê¸ì •" if pos > neg else "ë¶€ì •"


def classify_hybrid(text):
    rule = classify_sentiment(text)
    if rule == "ì¤‘ë¦½":
        return refine_with_plm(text)
    return rule

# %%


slang_cands = extract_slang_candidates(all_comments_labeled["text"], topn=200, min_freq=5)
weak_scores = weak_polarity_score(all_comments_labeled["text"], slang_cands)


all_comments_labeled["final_sentiment"] = all_comments_labeled["text"].map(classify_hybrid)

# ì €ì¥
all_comments_labeled.to_csv("data/meta/all_comments_final_sentiment.csv", index=False, encoding="utf-8-sig")
# %%
data = pd.read_csv("data/meta/all_comments_final_sentiment.csv")
print(data["final_sentiment"].value_counts())
# %%
print(data[data["final_sentiment"] == "ê¸ì •"]['text'])

## ì• ì´ˆì— ëŒ€ë¶€ë¶„ ê¸ì •ì´ê¸´í•¨ ê°ì • ë¶„ì„ì€ ì–´ë ¤ìš´ë“¯


# %% B) ì˜ìƒë³„ ëŒ“ê¸€ í‚¤ì›Œë“œ
INPUT_CSV = "data/meta/all_comments_final_sentiment.csv"
VIDEO_ID_COL = "video_id"
COMMENT_COL = "text"
LABEL_COL = "label_rule"
FONT_PATH = r"C:\Windows\Fonts\malgun.ttf"
TOP_K = 5

okt = Okt()

STOPWORDS_KO = {
    "ìˆë‹¤","ì•„ë‹ˆë‹¤","í•˜ë‹¤","ë˜ë‹¤","ì—†ë‹¤","ê·¸ë ‡ë‹¤","ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ë„ˆë¬´","ì§„ì§œ","ì •ë§",
    "ì´ê±°","ì €ê±°","ê·¸ê±°","ìš”ê±°","ê±°ê¸°","ì—¬ê¸°","ì €ê¸°"
}

def tokenize_ko(text: str):
    text = re.sub(r"[^ê°€-í£0-9a-zA-Z\s]", " ", str(text))
    return [w for w, p in okt.pos(text, stem=True, norm=True)
            if p in ("Noun", "Adjective") and len(w) >= 2 and w not in STOPWORDS_KO]


df = pd.read_csv(INPUT_CSV)

# ëŒ“ê¸€ ì»¬ëŸ¼/ë¹„ë””ì˜¤ID ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
assert VIDEO_ID_COL in df.columns, f"{VIDEO_ID_COL} ì»¬ëŸ¼ì´ ì—†ìŒ"
assert COMMENT_COL  in df.columns, f"{COMMENT_COL} ì»¬ëŸ¼ì´ ì—†ìŒ"

docs = (df.groupby(VIDEO_ID_COL)[COMMENT_COL]
          .apply(lambda x: " ".join(map(str, x)))
          .reset_index()
          .rename(columns={COMMENT_COL: "doc_text"}))


vectorizer = TfidfVectorizer(tokenizer=tokenize_ko, max_features=5000)
X = vectorizer.fit_transform(docs["doc_text"])
feature_names = vectorizer.get_feature_names_out()


top_keywords = []
X_csr = X.tocsr()
for i in range(X_csr.shape[0]):
    row = X_csr.getrow(i)
    if row.nnz == 0:
        top_keywords.append([])
        continue
    idx = row.indices[row.data.argsort()[::-1][:TOP_K]]
    kws = [feature_names[j] for j in idx]
    top_keywords.append(kws)

docs["top_keywords"] = top_keywords
print(docs[[VIDEO_ID_COL, "top_keywords"]].head(10))


if LABEL_COL is not None and LABEL_COL in df.columns:
    video_labels = df[[VIDEO_ID_COL, LABEL_COL]].drop_duplicates(VIDEO_ID_COL)
    docs = docs.merge(video_labels, on=VIDEO_ID_COL, how="left")

    label_keywords = {}
    for label, g in docs.groupby(LABEL_COL):
        c = Counter()
        for kws in g["top_keywords"]:
            c.update(kws)
        label_keywords[label] = [w for w, _ in c.most_common(5)]

    print("\në¼ë²¨ë³„ Top5 í‚¤ì›Œë“œ")
    for label, words in label_keywords.items():
        print(f"- {label}: {', '.join(words)}")
# %% ì˜ìƒë³„ ëŒ“ê¸€ ì¢‹ì•„ìš” í•©, ëŒ“ê¸€ í•© ì‹œê°í™”
from textwrap import shorten

df = pd.read_csv(INPUT_CSV)
df_t = pd.read_csv("data/meta/hwi_videos_labeled.csv", usecols=["video_id", "title"])
# ìœˆë„ìš° í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams["font.family"] = "Malgun Gothic"
plt.rcParams["axes.unicode_minus"] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€


for col in ["video_id", "comment_id", "like_count"]:
    assert col in df.columns, f"{col} ì»¬ëŸ¼ì´ ì—†ìŒ"
    
df["like_count"] = pd.to_numeric(df["like_count"], errors="coerce").fillna(0)

agg = (df.groupby("video_id")
       .agg(
           comment = ("comment_id", "nunique"),
           like = ("like_count", "sum"),
           first_published=("published_at", "min"),
       ).reset_index()
       )
agg["like_per_comment"] = agg["like"] / agg["comment"].fillna(0)

agg = agg.merge(df_t.drop_duplicates("video_id"), on="video_id", how="left")
# ë„ˆë¬´ ê¸´ ì œëª©ì€ ì˜ë¼ì„œ í‘œì‹œ (ì˜ˆ: 30ì)
agg["title_short"] = agg["title"].fillna("").apply(lambda s: shorten(str(s), width=30, placeholder="â€¦"))
topn = 20

print(agg.head())
## ì‹œê°í™” ëŒ“ê¸€ ìˆ˜ TopN
top_comments = agg.sort_values("comment", ascending=False).head(topn)
plt.figure(figsize=(12,6))
plt.bar(top_comments["title_short"].astype(str), top_comments["comment"])
plt.xticks(rotation=60, ha='right')
plt.ylabel("ëŒ“ê¸€ ìˆ˜")
plt.title(f"ì˜ìƒë³„ ëŒ“ê¸€ ìˆ˜ Top {topn}")
plt.tight_layout()
plt.show()

# ëŒ“ê¸€ ì¢‹ì•„ìš” í•©ê³„ TopN

top_likes = agg.sort_values("like", ascending=False).head(topn)
plt.figure(figsize=(12,6))
plt.bar(top_likes["title_short"].astype(str), top_likes["like"], color='orange')
plt.xticks(rotation=60, ha='right')
plt.ylabel("ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜")
plt.title(f"ì˜ìƒë³„ ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜ Top {topn}")
plt.tight_layout()
plt.show()

# ì‚°ì ë„
plt.figure(figsize=(6,5))
plt.scatter(agg["comment"], agg["like"])
plt.xlabel("ëŒ“ê¸€ ìˆ˜")
plt.ylabel("ëŒ“ê¸€ ì¢‹ì•„ìš” í•©ê³„")
plt.title("ëŒ“ê¸€ ìˆ˜ vs ëŒ“ê¸€ ì¢‹ì•„ìš” í•©ê³„ (ì˜ìƒë³„)")
plt.tight_layout()
plt.show()

# ëŒ“ê¸€ë‹¹ ì¢‹ì•„ìš” TopN (ëŒ“ê¸€ 5ê°œ ì´ìƒ)

top_lpc = (agg[agg["comment"] >= 5]
           .sort_values("like_per_comment", ascending=False)
           .head(topn))
plt.figure(figsize=(12,6))
plt.bar(top_lpc["title_short"].astype(str), top_lpc["like_per_comment"])
plt.xticks(rotation=60, ha="right")
plt.ylabel("like per comment")
plt.title(f"ëŒ“ê¸€ë‹¹ ì¢‹ì•„ìš” Top{topn} (ëŒ“ê¸€â‰¥5)")
plt.tight_layout()
plt.show()
# %%
# 3) ì¢‹ì•„ìš” ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì¶”ì¶œ
top5_comments = df.sort_values("like_count", ascending=False).head(5)

# 4) í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
print(top5_comments[["comment_id","video_id","author","text","like_count"]])



# %% ë¼ë²¨ë³„ ëŒ“ê¸€ ì¢‹ì•„ìš” í•©, ëŒ“ê¸€ í•© ì‹œê°í™”
agg_label = (df.groupby("label_rule")
       .agg(
           comment = ("comment_id", "nunique"),
           like = ("like_count", "sum"),
           video_count=("video_id","nunique"),
       ).reset_index()
       )
# í‰ê·  ëŒ“ê¸€ ìˆ˜, í‰ê·  ì¢‹ì•„ìš” ê³„ì‚°
agg_label["comment_mean"] = agg_label["comment"] / agg_label["video_count"]
agg_label["like_mean"] = agg_label["like"] / agg_label["video_count"]

# ë¼ë²¨ë³„ ëŒ“ê¸€ ìˆ˜ í‰ê· 
plt.figure(figsize=(12,6))
plt.bar(agg_label["label_rule"], agg_label["comment_mean"])
plt.xticks(rotation=60, ha='right')
plt.ylabel("ëŒ“ê¸€ ìˆ˜")
plt.title("ë¼ë²¨ë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜")
plt.tight_layout()
plt.show()

# ëŒ“ê¸€ ì¢‹ì•„ìš” í‰ê· 

plt.figure(figsize=(12,6))
plt.bar(agg_label["label_rule"], agg_label["like_mean"], color='orange')
plt.xticks(rotation=60, ha='right')
plt.ylabel("ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜")
plt.title("ë¼ë²¨ë³„ í‰ê·  ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜")
plt.tight_layout()
plt.show()



# %%
print(df['published_at'])
# %% ì‹œê³„ì—´ ê´€ë ¨ ë¶„ì„
# 1) ëŒ“ê¸€ì´ ëª°ë¦¬ëŠ” ì‹œê°„ëŒ€ ë¶„ì„
# 2) ëŒ“ê¸€ì´ ëª°ë¦¬ëŠ” ìš”ì¼ ë¶„ì„
df["published_at"] = pd.to_datetime(df["published_at"], errors="coerce", utc=True)

# í•œêµ­ ì‹œê°„ëŒ€(KST)ë¡œ ë§ì¶”ê¸°, +00:00 ì€ UTC ì˜¤í”„ì…‹ì„ ì˜ë¯¸
df["published_at"] = df["published_at"].dt.tz_convert("Asia/Seoul")

df["hour"] = df["published_at"].dt.hour
df["weekday"] = df["published_at"].dt.day_name()

# %%
hourly = df.groupby("hour")["comment_id"].count().reset_index()

pivot = df.pivot_table(index="weekday", columns="hour", values="comment_id", aggfunc="count", fill_value=0)

# ì‹œê°„ëŒ€ë³„ ë§‰ëŒ€ê·¸ë˜í”„
plt.figure(figsize=(10,5))
plt.bar(hourly["hour"], hourly["comment_id"])
plt.xticks(range(0,24))
plt.xlabel("ì‹œê°„ëŒ€ (ì‹œ)")
plt.ylabel("ëŒ“ê¸€ ìˆ˜")
plt.title("ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬")
plt.show()


# ìš”ì¼-ì‹œê°„ëŒ€ íˆíŠ¸ë§µ
plt.figure(figsize=(12,6))
sns.heatmap(pivot, cmap="YlOrRd")
plt.title("ìš”ì¼/ì‹œê°„ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ Heatmap")
plt.xlabel("ì‹œê°„ëŒ€ (ì‹œ)")
plt.ylabel("ìš”ì¼")
plt.show()

# %% ë‹¨ìˆœ í•©ì€ íŠ¹ì • ì—…ë¡œë“œ ìš”ì¼ì— ëª°ë¦´ìˆ˜ ìˆìŒ, í‰ê· ìœ¼ë¡œ ê³„ì‚°

df["weekday_num"] = df["published_at"].dt.dayofweek  # 0=ì›” ~ 6=ì¼
df["weekday_name"] = df["published_at"].dt.day_name()

# ê° ì˜ìƒ(video_id)ë³„ ëŒ“ê¸€ ìˆ˜
video_comments = df.groupby(["video_id","weekday_num","weekday_name"])["comment_id"].count().reset_index()

# ìš”ì¼ë³„ í‰ê·  (ì˜ìƒë‹¹ ëŒ“ê¸€ ìˆ˜)
weekday_avg = video_comments.groupby(["weekday_num","weekday_name"])["comment_id"].mean().reset_index()
weekday_avg = weekday_avg.sort_values("weekday_num")

# ì‹œê°í™”
plt.figure(figsize=(10,5))
plt.bar(weekday_avg["weekday_name"], weekday_avg["comment_id"])
plt.xlabel("ìš”ì¼")
plt.ylabel("í‰ê·  ëŒ“ê¸€ ìˆ˜ (ì˜ìƒë‹¹)")
plt.title("ìš”ì¼ë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜ ë¶„í¬")
plt.show()
# %%
